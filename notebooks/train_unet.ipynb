{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training Notebook for UNet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import sys\n","import os\n","sys.path.append(\"../\")\n","\n","if \"notebook\" in os.getcwd():\n","    os.chdir(\"../\")\n","\n","print(os.getcwd())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Change HYPER_OPT var below if doing Hyperparameter Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import traceback\n","import gc\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, random_split\n","from src.data.waveform_data import WaveformDataset\n","from skopt import gp_minimize\n","from skopt.space import Real, Integer, Categorical\n","from skopt.utils import use_named_args\n","from src.models.waveform.cicada_unet import CicadaUNetModel\n","\n","model = CicadaUNetModel()\n","\n","NOISY_DATA_PATH = f\"data/processed/28spk/combined_noisy_waves.pt\"\n","CLEAN_DATA_PATH = f\"data/processed/28spk/combined_clean_waves.pt\"\n","\n","batch_size = 32\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.manual_seed(42) #Consistent results\n","\n","\n","HYPER_OPT = False #Enable for Bayesian Hyperparameter Sweeps\n","f = 0.1 if HYPER_OPT else 1\n","num_epochs = 2 if HYPER_OPT else 30\n","lr = 1e-4 if HYPER_OPT else 7.96156607806295e-5\n","print(num_epochs, lr)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = WaveformDataset(NOISY_DATA_PATH, CLEAN_DATA_PATH, fraction=f)\n","\n","train_size = int(0.8 * len(data))\n","val_size = int(0.15 * len(data))\n","test_size = len(data) - train_size - val_size  # Ensure all samples are used\n","\n","train_set, val_set, test_set = random_split(data, [train_size, val_size, test_size])\n","print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n","\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["noisy_batch, clean_batch = next(iter(train_loader))\n","print(noisy_batch.shape, clean_batch.shape)  # Expected: [32, 1, 262144]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def count_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    \n","    print(f\"Total Parameters: {total_params:,}\")\n","    print(f\"Trainable Parameters: {trainable_params:,}\")\n","\n","    return total_params, trainable_params\n","    \n","count_parameters(model)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["t_losses = []\n","v_losses = []\n","snrs = []\n","\n","def compute_snr(clean, estimate):\n","    noise = clean - estimate\n","    snr = 10 * torch.log10(torch.sum(clean ** 2) / torch.sum(noise ** 2))\n","    return snr.item()\n","    \n","def train(model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=lr, n_encoders=None, s=None, k=None, device=device):\n","    model = CicadaUNetModel(n_encoders=n_encoders, s=s, k=k)\n","    model.to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    best_snr = -10\n","\n","    for epoch in range(num_epochs):\n","\n","        # Training\n","        print(\"Training ... \")\n","        model.train()\n","        train_loss = 0.0\n","        for noisy, clean in tqdm(train_loader):\n","            noisy, clean = noisy.to(device), clean.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(noisy)\n","            sample_loss = criterion(outputs, clean)\n","            sample_loss.backward()\n","            optimizer.step()\n","\n","            train_loss += sample_loss.item() * noisy.size(0)\n","        \n","        train_loss /= len(train_loader.dataset)\n","\n","        #Validation\n","        print(\"Evaluating ... \")\n","        model.eval()\n","        val_loss = 0.0\n","        total_snr_improvement = 0\n","        with torch.no_grad():  # No gradient computation\n","            for noisy, clean in tqdm(val_loader):\n","                noisy, clean = noisy.to(device), clean.to(device)\n","\n","                outputs = model(noisy)\n","                loss = criterion(outputs, clean)\n","                val_loss += loss.item() * noisy.size(0)\n","\n","                #Computer SNR for evaluation\n","                snr_noisy = compute_snr(clean, noisy)\n","                snr_output = compute_snr(clean, outputs)\n","\n","                # Compute SNR improvement\n","                snr_improvement = snr_output - snr_noisy\n","                total_snr_improvement += snr_improvement * noisy.size(0)\n","\n","            val_loss /= len(val_loader.dataset)  # Average loss\n","            avg_snr_improvement = total_snr_improvement / len(val_loader.dataset)\n","            if avg_snr_improvement > best_snr:\n","                best_snr = avg_snr_improvement\n","                if not HYPER_OPT: # Save Best CKPT\n","                    print(\"SAVING\")\n","                    torch.save(model.state_dict(), f\"ckpts/cicadence_unet_epoch_{epoch}.pt\")\n","                \n","        t_losses.append(train_loss)\n","        v_losses.append(val_loss)\n","        snrs.append(avg_snr_improvement)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | SNR Improvement: {avg_snr_improvement:.2f} dB\")\n","\n","    print(\"Training Complete!\")\n","    if HYPER_OPT:\n","        return val_loss\n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Set Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def objective(params):\n","    n_encoders = params[0]\n","    s_start = params[1]\n","    kernel = params[2]\n","    lr = params[3]\n","    offsets = params[4:]\n","    s = [1]\n","    # Introduce variability in hidden sizes of encoder/decoder blocks\n","    for i in range(n_encoders):\n","        s.append(s_start)\n","        if s_start * 2 - offsets[i] <= s_start:\n","            s_start *= 2\n","        else:\n","            s_start = s_start * 2 - offsets[i]\n","    k = [kernel for i in range(n_encoders)]\n","    print(n_encoders, s)\n","    print(f\"LR: {lr}\")\n","    print(f\"Kernel: {kernel}\")\n","    return train(model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=lr, n_encoders=n_encoders, s=s, k=k)\n","    \n","if HYPER_OPT:\n","    space = [\n","        Integer(3, 7, name='n_encoders'),\n","        Integer(2, 16, name='s_start'),    # Starting hidden size\n","        Categorical([3, 5, 7], name='kernel_size'),\n","        Real(1e-5, 1e-3, prior='log-uniform', name='learning_rate'),\n","        *[Integer(1, 10, name=f's_step{i}') for i in range(1, 11)]\n","    ]\n","        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Perform Training or Hyperparameter Sweeps"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["try:\n","    if HYPER_OPT:\n","        results = gp_minimize(objective, space, n_calls=25, random_state=42)\n","        print(\"Best hyperparameters:\")\n","        for dim, val in zip(space, results.x):\n","            print(f\"{dim.name}: {val}\")\n","    else:\n","        train(model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=lr, device=device)\n","except Exception as e:\n","    print(e)\n","    traceback.print_exc()\n","finally:\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(t_losses)\n","print(v_losses)\n","print(snrs)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6837736,"sourceId":11002768,"sourceType":"datasetVersion"},{"datasetId":6837748,"sourceId":11003356,"sourceType":"datasetVersion"},{"datasetId":6896027,"sourceId":11066583,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"ActualAudioENV","language":"python","name":"audioenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10 (v3.8.10:3d8993a744, May  3 2021, 09:09:08) \n[Clang 12.0.5 (clang-1205.0.22.9)]"}},"nbformat":4,"nbformat_minor":4}
